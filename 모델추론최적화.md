# AI 모델 추론 성능 최적화 보고서

## 1. 개요

AI 기반 커리어 탐색 서비스의 핵심 기능인 CS 면접 답변에 대한 피드백 생성 과정에서 모델의 추론 성능 최적화는 필수적이다. 현재 사용 중인 AI 모델의 추론 성능 분석을 통해 병목 요소를 파악하고, 서비스 맥락에 적합한 최적화 기법을 적용하는 계획을 설명한다.

## 2. 모델 성능 비교

| 모델명 | Hugging Face 경로 | 속도 (tokens/sec) | VRAM 사용량 (MB) |
|--------|-------------------|-------------------|------------------|
| Mistral-7B-Instruct-v0.3 | mistralai/Mistral-7B-Instruct-v0.3 | 19.03 | 4496.57 |
| DeepSeek-V2 Chat 7B | deepseek-ai/DeepSeek-V2-Chat | - | - |
| Qwen2.5-7B-Instruct | Qwen/Qwen2.5-7B-Instruct | 18.68 | 5564.99 |
| LLaMA-3-8B Instruct | meta-llama/Meta-Llama-3-8B-instruct | 18.638 | 6047.93 |
| gemma-2-9b-it | google/gemma-2-9b-it | 10.17 | 6867.19 |
| gemma-ko-7b | beomi/gemma-ko-7b | 27.61 | 17083.89 |
| Llama-2-ko-7b-Chat | kfkas/Llama-2-ko-7b-Chat | 27.11 | 13720.25 |
| KoAlpaca Polyglot 12.8B | beomi/KoAlpaca-Polyglot-12.8B | 20.62 | 7706.31 |

## 3. 선정 모델 성능 지표

환경적 제약(월 예산 10~15만원, AWS L4 GPU 사용)으로 인해 상대적으로 경량이면서도 성능이 우수한 모델이 필요함.

**선택된 모델: Mistral-7B-Instruct-v0.3**

- 평균 지연 시간: 약 1.7초 (500 tokens 기준)
- 처리량: 약 0.58 req/sec (35 req/min)
- GPU 사용률: 약 45~60%
- 메모리 사용률: 약 4.5GB (L4 GPU 24GB 기준 약 20%)

## 4. 식별된 성능 병목 요소와 원인 분석

- **토큰 처리 속도 한계**
  - 증상: 사용자 입력이 길어질수록 응답 지연 발생
  - 원인: 평균 19 tokens/sec 속도에서 프롬프트 + 출력 합산 토큰이 커지면 지연 증가

- **동시 요청 시 GPU 자원 경쟁**
  - 증상: 다중 요청 시 추론 병렬화가 되지 않아 지연 발생
  - 원인: GPU는 기본적으로 단일 추론 연산만 처리

- **Input/Output 처리 지연**
  - 증상: 프롬프트 구성 또는 결과 후처리 지연
  - 원인: Tokenizer 처리 지연, 출력 텍스트 후처리 병목

## 5. 최적화 기법 계획

### 5.1 양자화(Quantization)
- **이유**: 추론 속도 및 메모리 절감, 비용 효율
- **방법**: bitsandbytes 활용, INT4 / INT8 양자화 적용
- **기대 효과**: GPU 부하 감소(예: 80%→40%), 속도 향상(예: 3.5초→1.2초), 메모리 최대 50% 절감

### 5.2 LoRA (Low-Rank Adaptation)
- **이유**: 도메인 특화 파인튜닝에 최적, 학습 효율 증가
- **방법**: PEFT 라이브러리, Rank=16, 소규모 파인튜닝
- **기대 효과**: 정확도 향상, 비용 절감, 전체 모델 재학습 대비 효율적

### 5.3 KV-Cache 최적화
- **이유**: 실시간 피드백 응답 속도 개선
- **방법**: use_cache=True 설정, GPU 내 Key/Value 캐싱
- **기대 효과**: 추론 속도 최대 2~4배 향상

### 5.4 FlashAttention
- **이유**: 긴 입력 시 Attention 연산 최적화
- **방법**: 모델별 지원 여부 확인 (Mistral 지원 O, gemma는 커스텀 필요)
- **기대 효과**: 메모리 절감 + 응답 지연 완화

## 6. 최적화 적용 후 기대 성능 지표

| 지표 | 최적화 전 | 최적화 후 | 개선율 |
|------|-----------|-----------|--------|
| 평균 추론 시간 | 3.5초 | 1.8초 | 약 50% ↓ |
| 초당 처리량 | 0.29 req/sec | 0.56 req/sec | 약 90% ↑ |
| GPU 사용량 | 평균 80% | 평균 40% | 약 25% ↓ |
| 메모리 사용량 | 14GB | 8.5GB | 약 40% ↓ |

## 7. 결론

제안된 최적화 기법들을 통해 AI 모델의 추론 성능을 획기적으로 개선할 수 있으며, 사용자 경험 향상과 인프라 비용 절감이라는 목표를 달성할 것으로 기대한다.