from langchain_community.llms import HuggingFacePipeline
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import fitz


# fitzë¥¼ ì´ìš©í•´ PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
doc = fitz.open("ì½”ë”©ëª¬ìŠ¤í„°-í‘œì¤€ì´ë ¥ì„œ.pdf")
text = "\n".join([page.get_text() for page in doc])

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ë¡œê·¸ì¸ í•„ìš” ì‹œ Hugging Face í† í° í¬í•¨)
model_name = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", torch_dtype="auto", use_auth_token=True
)

# HF íŒŒì´í”„ë¼ì¸ ìƒì„±
mistral_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    do_sample=False,
)


# LangChainì˜ HuggingFacePipelineë¡œ ë˜í•‘
llm = HuggingFacePipeline(pipeline=mistral_pipeline)


# PromptTemplate êµ¬ì„±
template = """
ë‹¤ìŒì€ ì´ë ¥ì„œ í…ìŠ¤íŠ¸ì•¼. ì´ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜:

- ì´ë©”ì¼ ì£¼ì†Œ (ì—†ìœ¼ë©´ null)
- í”„ë¡œì íŠ¸ ê°œìˆ˜
- ìê²©ì¦ ê°œìˆ˜
- ì´ ê²½ë ¥ ê¸°ê°„ (ê°œì›” ë‹¨ìœ„, ì—†ìœ¼ë©´ 0)

ì•„ë˜ ì´ë ¥ì„œë¥¼ ë¶„ì„í•´ì¤˜:
-------------------------
{text}
-------------------------
"""

prompt = PromptTemplate(input_variables=["text"], template=template)
chain = LLMChain(llm=llm, prompt=prompt)

# ì‹¤í–‰
result = chain.run(text=text)
print("ğŸ” ì¶”ì¶œ ê²°ê³¼:")
print(result)
